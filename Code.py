# -*- coding: utf-8 -*-
"""HW1_Electronic_Commerce_updated.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CQ3dygWnRiLN0jv3Wd92sZK0fgDXsNCe

# HW1 Electronic Commerce

Matan Birnboim 


## Imports and Setup
"""

import numpy as np
import networkx as nx
import random
import pandas as pd
from pip import main

def choose_artists():

  """
  Enter your ids below (if you are submitting alone DO NOT CHANGE ID2) and execute the code.
  The list of ids you get is the list of artists you need to promote.
  """

  ####################
  # TODO: change this
  ID1 = 
  ID2 = 
  ####################

  x = (int(ID1[-1]) + int(ID2[-1])) % 5
  y = (int(ID1[-2]) + int(ID2[-2])) % 5
  options = [(70, 150), (989, 16326), (144882, 194647), (389445, 390392), (511147, 532992)]
  y = (y + 1) % 5 if x == y else y
  print("your artists are:")
  artists = [*options[x], *options[y]]
  print(artists)

def edge_creation_probs(not_created_and_common, created_and_common):
  not_created_df = pd.DataFrame(not_created_and_common, columns=['edge', 'common nei num'])
  created_df = pd.DataFrame(created_and_common, columns=['edge', 'common nei num'])

  not_created_count = not_created_df.groupby(['common nei num'])['common nei num'].count().to_frame()
  created_count = created_df.groupby(['common nei num'])['common nei num'].count().to_frame()

  not_created_count = not_created_count.rename({'common nei num': 'not_created_cnt'}, axis='columns')
  created_count = created_count.rename({'common nei num': 'created_cnt'}, axis='columns')
  joined_nei_cnt = not_created_count.join(created_count)
  joined_nei_cnt['created_cnt'] = joined_nei_cnt['created_cnt'].fillna(0)
  joined_nei_cnt['edge_prob'] = joined_nei_cnt['created_cnt'] / (joined_nei_cnt['created_cnt'] + joined_nei_cnt['not_created_cnt'])
  joined_nei_cnt = joined_nei_cnt.drop(['created_cnt', 'not_created_cnt'], axis=1)
  output = joined_nei_cnt.to_dict()
  return output['edge_prob']

def find_nearest(array, value):
    array = np.asarray(array)
    idx = (np.abs(array - value)).argmin()
    return array[idx]

def create_edges(G, probs_dict):
  node_with_new_neighbors = []
  can_be_created = list(nx.difference(nx.complete_graph(G), G).edges) # Find all edges that had not been created yet
  not_created = can_be_created.copy()
  created = []
  
  for edge in can_be_created:
    common_nei_num = len(set(nx.neighbors(G, edge[0])).intersection(set(nx.neighbors(G, edge[1])))) # Number of common neighbors between the 2 edge nodes
    common_nei_closest = find_nearest(list(probs_dict.keys()), common_nei_num) # For every number of edges that is not on the probability dictionary, assign the probability of the nearest number of neighbors value
    indc = np.random.rand() # Simulate the probability
    if indc < probs_dict[common_nei_closest]:
      node_with_new_neighbors.append(edge[0]) # Append both nodes to the list of nodes with new neighbors
      node_with_new_neighbors.append(edge[1])
      G.add_edge(edge[0], edge[1]) # add the edge to the graph
      created.append(edge)
      not_created.remove(edge)
  
  not_created_and_common = [(edge, len(set(nx.neighbors(G, edge[0])).intersection(set(nx.neighbors(G, edge[1]))))) for edge in not_created] # [(edge, number of common neighbors between the edge nodes),.... for every edge that was not created]
  created_and_common = [(edge, len(set(nx.neighbors(G, edge[0])).intersection(set(nx.neighbors(G, edge[1]))))) for edge in created] # [(edge, number of common neighbors between the edge nodes),.... for every edge that was created]
  return node_with_new_neighbors, edge_creation_probs(not_created_and_common, created_and_common)

"""# Conclusion
The improved triadic closure and the logistic regression model produced similar accuracy on the given graphs with a slight advantage to the improved triadic closure. Therfore, we chose to use the improved triadic closure method which also was much faster.

## Influencers selection model
We will use the greedy hill climbing algorithm as follows:

- Define the final influencers list
- for influencer_num in 1:5:

  A. for person in graph:
    1. infect the person
    2. for every simulation:

      2.1. for t in 1:6:

          - Update Bt and Nt for all nodes

          - Start infection proccess for the current time according to the probability mentioned in the exercise

          - Create edges according to the edge creation model

  B. Choose the person with that produces the biggest number of infeted nodes and append him to the influencers list
"""

def initialize(G_0, G_minus_1):
  not_created = list(nx.difference(nx.complete_graph(G_0.nodes), G_0).edges) # List of edges that had not been created between times -1 and 0
  created = list(nx.difference(G_0, G_minus_1).edges) # List of new edges between times -1 and 0

  not_created_and_common = [(edge, len(set(nx.neighbors(G_minus_1, edge[0])).intersection(set(nx.neighbors(G_minus_1, edge[1]))))) for edge in not_created]
  created_and_common = [(edge, len(set(nx.neighbors(G_minus_1, edge[0])).intersection(set(nx.neighbors(G_minus_1, edge[1]))))) for edge in created]
  return edge_creation_probs(not_created_and_common, created_and_common)

def infect(G, artist):
    infected_nodes = []
    for person in list(G.nodes()): # Run over all nodes and try to infect them
        if G.nodes[person]['bought']: continue # Each person can buy only once
        prob = float(G.nodes[person]['Bt']) / float(G.nodes[person]['Nt']) # The default probability is Bt/Nt
        if G.nodes[person]['h'][artist] > 0:
            prob = prob * float(G.nodes[person]['h'][artist]) / 1000 # If the person listened to the artist's songs' the probability should be Bt*h/(Nt*1000)
        rnd = np.random.rand() # Simulate probability
        if rnd <= prob:  
            G.nodes[person]['bought'] = True # infect the person
            infected_nodes.append(person) # add the person to the newly infected list
    return infected_nodes

"""### Simplifying Assumption
After multiple attemps of running the code we noticed that an insignificant number of edges is created in each iteration of every simulation in the graph. Furthermore, the edge creation proccess was way too long to iterate over all people 5 times for every artist. So we decided to ignore the edge creation proccess and apply only the infection proccess which highly improved the running time of the script.

#### Code witout edge creation (That we ran in practice)
"""

def find_my_influencers(artists, G_0, G_minus_1, nodes_with_new_neighbors_init):
  output = {artist: [] for artist in artists}
  for artist in artists: # Choose influencers for every artist
    influencers = []
    num_simulations = 10
    probs_0 = initialize(G_0, G_minus_1) # Initialize the probabilities for edge creation according to the graph in time 0
    for inf_num in range(5): # Find 5 best influencers
        print(f"artist: {artist}, influ: {inf_num}")
        best_current_influencer = 0
        max_infection = 0
        person_list = list(G_0.nodes())
        random.shuffle(person_list)
        i = 0
        for person in person_list: # Out of all people find the current best influencer combined with the influencers that were already chosen
            if i % 100 == 0:
              print(i)
            i += 1
            if person in influencers: continue # If we encounter a person that was already chosen, skip him
            G_t = G_0.copy() # Create a new copy of the graph in time 0
            G_t.nodes[person]['bought'] = True # Infect the current person checked

            for nei in list(G_t.neighbors(person)): # Infecting all neighbors of the current person
                G_t.nodes[nei]['Bt'] += 1

            all_sim_infection_sum = 0 # Sum the number of infected people by the checked person in all simulations

            nodes_with_new_neighbors = nodes_with_new_neighbors_init # All nodes with new neighbors between times -1 and 0
            for sim in range(num_simulations): # The proccess is stochastic, therefore, we run multiple simulations to get closer to the expectation
              probs_t = probs_0.copy()
              for t in range(6): # Simulate the infection proccess combined with the edge creation proccess
                  for node in nodes_with_new_neighbors: # Update Nt  of all relevant nodes
                      G_t.nodes[node]['Nt'] = G_t.nodes[node]['Nt'] + 1

                  for node in infect(G_t, artist): # Update Bt of all nodes and infect
                    G_t.nodes[node]['Bt'] = sum([G_t.nodes[nei]['bought'] for nei in list(G_t.neighbors(node))])
                    
                  nodes_with_new_neighbors, probs_t = create_edges(G_t, probs_t)
              all_sim_infection_sum += sum([G_t.nodes[person]['bought'] for person in list(G_t.nodes())])

            if max_infection < all_sim_infection_sum / num_simulations:  # If the current person achieved better average infection than the current best, make him the current best influencer
                best_current_influencer = person
                max_infection = all_sim_infection_sum / num_simulations
        print(best_current_influencer)
        influencers.append(best_current_influencer) # Add the best 
        G_0.nodes[best_current_influencer]['bought'] = True
    for inf in influencers: # return the graph at time 0 to the initial state (in term of infected nodes)
      G_0.nodes[best_current_influencer]['bought'] = False
    output[artist] = influencers
  return output
        
def main():
  artists = choose_artists()
  inst_minus_1 = pd.read_csv('instaglam_1.csv')
  inst_0 = pd.read_csv('instaglam0.csv')
  users = np.unique(inst_0['userID'].to_numpy(dtype=str))
  users_num = users.shape[0]
  
  # Create graphs for times -1 and 0
  G_minus_1 = nx.parse_edgelist(list(map(lambda pair: f"{pair[0]} {pair[1]}",inst_minus_1.values.tolist())), nodetype=str)
  G_0 = nx.parse_edgelist(list(map(lambda pair: f"{pair[0]} {pair[1]}",inst_0.values.tolist())), nodetype=str)

  inst_minus_1 = pd.read_csv('instaglam_1.csv')
  inst_0 = pd.read_csv('instaglam0.csv')
  users = np.unique(inst_0['userID'].to_numpy(dtype=str))
  users_num = users.shape[0]


  # Create graphs for times -1 and 0
  G_minus_1 = nx.parse_edgelist(list(map(lambda pair: f"{pair[0]} {pair[1]}",inst_minus_1.values.tolist())), nodetype=str)
  G_0 = nx.parse_edgelist(list(map(lambda pair: f"{pair[0]} {pair[1]}",inst_0.values.tolist())), nodetype=str)

  artists = [70, 150, 989, 16326]
  nx.set_node_attributes(G_0, 0, name='Nt') # Number of neighbors in time t
  nx.set_node_attributes(G_0, 0, name='Bt') # Number of neighbors who have the 
  nx.set_node_attributes(G_0, {}, name='h')
  nx.set_node_attributes(G_0, False, name='bought')

  df = pd.read_csv('spotifly.csv')
  df['tup'] = list(zip(df[' artistID'], df['#plays']))
  spotifly_dict = {k: dict(v) for k, v in df.groupby('userID')['tup'].apply(list).to_dict().items()}
  def update_nodes_attributes(G_t):
      Bt_dict = {node: sum(
              [G_t.nodes[nei]['bought'] for nei in list(nx.neighbors(G_t, node))])
        for node in list(G_t.nodes)}
      nx.set_node_attributes(G_t, Bt_dict, name='Bt')

  update_nodes_attributes(G_0)

  for node in list(G_0.nodes):
      G_0.nodes[node]['Nt'] = len(list(G_0.neighbors(node)))
      G_0.nodes[node]['h'] = {
          artist: spotifly_dict[int(node)][artist] if artist in spotifly_dict[int(node)].keys() else 0 for artist in artists}
  
  nodes_with_new_neighbors_init = list(sum(list(nx.difference(G_0, G_minus_1).edges()),()))
  
  output = find_my_influencers(artists, G_0, G_minus_1, nodes_with_new_neighbors_init)

  with open('ID1_ID2.csv', 'w') as f:
      f.write("artist Id,influencer 1,influencer 2,influencer 3,influencer 4,influencer 5\n")
      for key in output.keys():
          print(key, output[key])
          f.write("%s,%s,%s,%s,%s,%s\n"%(key,output[key][0],output[key][1],output[key][2],output[key][3],output[key][4]))


if __name__ == "__main__":
    main()
